{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1: Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "\n",
    "wb1 = xlrd.open_workbook(r'covid_data_0.xlsx')\n",
    "sheets_name1 = wb1.sheet_names();\n",
    "ws1 = wb1.sheet_by_index(0)\n",
    "true_tweets = ws1.col_values(0)\n",
    "\n",
    "wb2 = xlrd.open_workbook(r'covid_data_1.xlsx')\n",
    "sheets_name2 = wb1.sheet_names();\n",
    "ws2 = wb2.sheet_by_index(0)\n",
    "misinfo_tweets = ws2.col_values(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(misinfo_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'misinfo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-ee1cfbe51145>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmisinfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'misinfo' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(true_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2: Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mprit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_tweets_original = []\n",
    "true_tweet_tokens = []\n",
    "for i in range(len(true_tweets)):\n",
    "    temp_split = nltk.word_tokenize(true_tweets[i])\n",
    "    true_tweet_tokens.append(temp_split)\n",
    "    true_tweets_original.append(true_tweets[i])\n",
    "\n",
    "misinfo_tweets_original= []\n",
    "misinfo_tweet_tokens = []\n",
    "for i in range(len(misinfo_tweets)):\n",
    "    temp_split = nltk.word_tokenize(misinfo_tweets[i])\n",
    "    misinfo_tweet_tokens.append(temp_split)\n",
    "    misinfo_tweets_original.append(misinfo_tweets[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mprit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mprit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\mprit\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: click in c:\\users\\mprit\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\mprit\\anaconda3\\lib\\site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mprit\\anaconda3\\lib\\site-packages (from nltk) (4.47.0)\n",
      "Requirement already satisfied: regex in c:\\users\\mprit\\anaconda3\\lib\\site-packages (from nltk) (2020.6.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_stop_words = ['....',\n",
    "'...',\n",
    "'around','covidãƒ¼19',\n",
    "'12','60','14','5','//t.câ€¦', \"'m\",\"'re\",'â€”','â€“','..','2', '20','4','//t.â€¦','aâ€¦','bâ€¦','câ€¦','fâ€¦','itâ€¦','noâ€¦','theâ€¦','covid_19',\n",
    "'200,000','dâ€¦','isâ€¦','na','sâ€¦','tâ€¦','b','u','19.', 'https','1.7', '50',\n",
    "'thâ€¦',\n",
    "'3',\n",
    "'19',\n",
    "'100',\n",
    "\"'if\",\n",
    "'â€¦','s',\n",
    "'t','y',\n",
    "\"''\",\n",
    "\"'s\",\n",
    "'``',\n",
    "\"n't\",\n",
    "\"'d\",\n",
    "'d',\n",
    "'â€™',\n",
    "'â€˜',\n",
    "'â€',\n",
    "'â€œ',\n",
    "'â€',\n",
    "'â€œ',\n",
    "'covid19',\n",
    "'covid-19',\n",
    "'coronavirus',    \n",
    "'corona',    \n",
    "'a', # start of initial stop words\n",
    "'able',\n",
    "'about',\n",
    "'across',\n",
    "'after',\n",
    "'all',\n",
    "'almost',\n",
    "'also',\n",
    "'am',\n",
    "'among',\n",
    "'an',\n",
    "'and',\n",
    "'any',\n",
    "'are',\n",
    "'as',\n",
    "'at',\n",
    "'be',\n",
    "'because',\n",
    "'been',\n",
    "'but',\n",
    "'by',\n",
    "'can',\n",
    "'cannot',\n",
    "'could',\n",
    "'dear',\n",
    "'did',\n",
    "'do',\n",
    "'does',\n",
    "'either',\n",
    "'else',\n",
    "'ever',\n",
    "'every',\n",
    "'for',\n",
    "'from',\n",
    "'get',\n",
    "'got',\n",
    "'had',\n",
    "'has',\n",
    "'have',\n",
    "'he',\n",
    "'her',\n",
    "'hers',\n",
    "'him',\n",
    "'his',\n",
    "'how',\n",
    "'however',\n",
    "'i',\n",
    "'if',\n",
    "'in',\n",
    "'into',\n",
    "'is',\n",
    "'it',\n",
    "'its',\n",
    "'just',\n",
    "'least',\n",
    "'let',\n",
    "'like',\n",
    "'likely',\n",
    "'may',\n",
    "'me',\n",
    "'might',\n",
    "'most',\n",
    "'must',\n",
    "'my',\n",
    "'neither',\n",
    "'no',\n",
    "'nor',\n",
    "'not',\n",
    "'of',\n",
    "'off',\n",
    "'often',\n",
    "'on',\n",
    "'only',\n",
    "'or',\n",
    "'other','our','own','rather','said','say','says','she','should','since','so','some','than','that','the','their','them','then',\n",
    "'there','these','they','this','tis','to','too','twas','us','wants','was','we','were','what','when','where','which','while','who','whom',\n",
    "'why','will','with','would','yet','you','your', 'ðŸ™„', \"'ve\",'@','rt','http','\"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(our_stop_words)\n",
    "# initial size 125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Remove noise, stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#:/]|[!*\\(\\),]|...\"ðŸ˜·RT'\\\n",
    "                       'http(?:%[0-9a-fA-F][0-9a-fA-F]))+\"...\"ðŸ˜·RT','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_cleaned_tokens_list = []\n",
    "misinfo_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in true_tweet_tokens:\n",
    "    true_cleaned_tokens_list.append(remove_noise(tokens, our_stop_words))\n",
    "\n",
    "for tokens in misinfo_tweet_tokens:\n",
    "    misinfo_cleaned_tokens_list.append(remove_noise(tokens, our_stop_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting tokens\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_true_tweet_words = get_all_words(true_cleaned_tokens_list)\n",
    "all_misinfo_tweet_words = get_all_words(misinfo_cleaned_tokens_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "freq_dist_true_tweet = FreqDist(all_true_tweet_words)\n",
    "freq_dist_misinfo_tweet = FreqDist(all_misinfo_tweet_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mask', 74), ('virus', 71), ('stay', 66), ('lockdown', 59), ('home', 52), ('wear', 44), ('health', 39), ('vaccine', 33), ('people', 33), ('supportlockdownstaysafe', 31), ('spread', 31), ('covid', 29), ('test', 26), ('please', 25), ('social', 24), ('more', 23), ('pandemic', 21), ('distancing', 20), ('fight', 20), ('amp', 20), ('go', 19), ('safe', 18), ('stop', 17), ('good', 17), ('one', 16), ('person', 16), ('now', 16), ('patient', 16), ('case', 15), ('against', 15), ('life', 15), ('very', 15), ('public', 15), ('still', 15), ('new', 15), ('follow', 14), ('hand', 14), ('use', 14), ('healthy', 14), ('protect', 13), ('time', 13), ('india', 12), ('infection', 12), ('take', 12), ('way', 11), ('make', 11), ('infect', 11), ('disease', 11), ('drug', 11), ('antibody', 10)]\n"
     ]
    }
   ],
   "source": [
    "print(freq_dist_true_tweet.most_common(50))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('test', 40), ('virus', 34), ('covid', 32), ('hydroxychloroquine', 32), ('mask', 29), ('use', 29), ('drug', 27), ('now', 27), ('trump', 23), ('vaccine', 23), ('case', 22), ('emergency', 21), ('stop', 19), ('fda', 18), ('very', 17), ('wear', 17), ('people', 17), ('health', 17), ('5g', 16), ('right', 16), ('few', 15), ('amp', 13), ('one', 12), ('make', 12), ('treat', 12), ('patient', 12), ('lockdown', 11), ('treatment', 11), ('authorization', 11), ('new', 11), ('up', 10), ('immunity', 10), ('good', 10), ('revoke', 10), ('more', 10), ('stay', 9), ('need', 9), ('go', 9), ('see', 9), ('take', 9), ('actually', 8), ('doctor', 8), ('social', 8), ('pandemic', 8), ('time', 8), ('flu', 8), ('chloroquine', 8), ('against', 8), ('malaria', 8), ('rate', 8)]\n"
     ]
    }
   ],
   "source": [
    "print(freq_dist_misinfo_tweet.most_common(50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_tweet_tokens_for_model = get_tweets_for_model(true_cleaned_tokens_list)\n",
    "misinfo_tweets_tokens_for_model = get_tweets_for_model(misinfo_cleaned_tokens_list)\n",
    "\n",
    "import random\n",
    "\n",
    "true_tweet_dataset = [(tweet_dict, \"0\")\n",
    "                     for tweet_dict in true_tweet_tokens_for_model]\n",
    "\n",
    "misinfo_tweet_dataset = [(tweet_dict, \"1\")\n",
    "                     for tweet_dict in misinfo_tweets_tokens_for_model]\n",
    "\n",
    "#Merge_dataset = true_tweet_dataset + misinfo_tweet_dataset\n",
    "\n",
    "\n",
    "#random.shuffle(Merge_dataset)\n",
    "\n",
    "#train_data2 = dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_tweet_df = pd.DataFrame(true_tweet_dataset, columns=['tokens', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "misinfo_tweet_df = pd.DataFrame(misinfo_tweet_dataset, columns=['tokens', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'india': True, 'potentially': True, 'large': ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'scientist': True, 'medical': True, 'research...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'save': True, 'urself': True, 'whole': True, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'follow': True, 'social': True, 'distancing':...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens Label\n",
       "0  {'india': True, 'potentially': True, 'large': ...     0\n",
       "1  {'scientist': True, 'medical': True, 'research...     0\n",
       "2  {'save': True, 'urself': True, 'whole': True, ...     0\n",
       "3  {'follow': True, 'social': True, 'distancing':...     0"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_tweet_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_tweet_df['Text'] = true_tweets_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "misinfo_tweet_df['Text'] = misinfo_tweets_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'test': True, 'virus': True, 'pick': True, 'u...</td>\n",
       "      <td>1</td>\n",
       "      <td>if the test for corona virus is picking up exo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'nigga': True, 'cause': True, '5g': True, 'ne...</td>\n",
       "      <td>1</td>\n",
       "      <td>niggas said corona is caused from 5g network r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'donald': True, 'trump': True, 'approve': Tru...</td>\n",
       "      <td>1</td>\n",
       "      <td>donald trump approved 5g so he is just as evil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'something': True, 'abt': True, 'government':...</td>\n",
       "      <td>1</td>\n",
       "      <td>they saying something abt the government insta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens Label  \\\n",
       "0  {'test': True, 'virus': True, 'pick': True, 'u...     1   \n",
       "1  {'nigga': True, 'cause': True, '5g': True, 'ne...     1   \n",
       "2  {'donald': True, 'trump': True, 'approve': Tru...     1   \n",
       "3  {'something': True, 'abt': True, 'government':...     1   \n",
       "\n",
       "                                                Text  \n",
       "0  if the test for corona virus is picking up exo...  \n",
       "1  niggas said corona is caused from 5g network r...  \n",
       "2  donald trump approved 5g so he is just as evil...  \n",
       "3  they saying something abt the government insta...  "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misinfo_tweet_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_tweets_data = pd.concat([true_tweet_df, misinfo_tweet_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(525, 3)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_tweets_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_tweets_data = sklearn.utils.shuffle(merge_tweets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'two': True, 'oxford': True, 'professor': Tru...</td>\n",
       "      <td>1</td>\n",
       "      <td>two oxford professors and one at dundee have s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>{'read': True, 'fake': True, 'news': True, 'wh...</td>\n",
       "      <td>1</td>\n",
       "      <td>read that some fake news whatsapp forward talk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>{'still': True, 'fight': True, 'pandemic': Tru...</td>\n",
       "      <td>0</td>\n",
       "      <td>we are still fighting a pandemic. and covid-19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>{'everyone': True, 'flu': True, 'shot': True, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>everyone that had a flu shot will test positiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>{'immunity': True, 'important': True, 'fight':...</td>\n",
       "      <td>0</td>\n",
       "      <td>immunity is important in the fight against cor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tokens Label  \\\n",
       "15   {'two': True, 'oxford': True, 'professor': Tru...     1   \n",
       "169  {'read': True, 'fake': True, 'news': True, 'wh...     1   \n",
       "133  {'still': True, 'fight': True, 'pandemic': Tru...     0   \n",
       "168  {'everyone': True, 'flu': True, 'shot': True, ...     1   \n",
       "131  {'immunity': True, 'important': True, 'fight':...     0   \n",
       "\n",
       "                                                  Text  \n",
       "15   two oxford professors and one at dundee have s...  \n",
       "169  read that some fake news whatsapp forward talk...  \n",
       "133  we are still fighting a pandemic. and covid-19...  \n",
       "168  everyone that had a flu shot will test positiv...  \n",
       "131  immunity is important in the fight against cor...  "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_tweets_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-205-05fb8173106e>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merge_tweets_data['Text_Clean'] = merge_tweets_data['Text'].apply(lambda x: remove_punct(x))\n"
     ]
    }
   ],
   "source": [
    "def remove_punct(text):\n",
    "    text_nopunct = ''\n",
    "    text_nopunct = re.sub('['+string.punctuation+']', '', text)\n",
    "    return text_nopunct\n",
    "\n",
    "merge_tweets_data['Text_Clean'] = merge_tweets_data['Text'].apply(lambda x: remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_Clean</th>\n",
       "      <th>Text_Final</th>\n",
       "      <th>true_label</th>\n",
       "      <th>misinfo_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'two': True, 'oxford': True, 'professor': Tru...</td>\n",
       "      <td>1</td>\n",
       "      <td>two oxford professors and one at dundee have s...</td>\n",
       "      <td>two oxford professors and one at dundee have s...</td>\n",
       "      <td>two oxford professors one dundee scientific ev...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>{'read': True, 'fake': True, 'news': True, 'wh...</td>\n",
       "      <td>1</td>\n",
       "      <td>read that some fake news whatsapp forward talk...</td>\n",
       "      <td>read that some fake news whatsapp forward talk...</td>\n",
       "      <td>read fake news whatsapp forward talks drinking...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>{'still': True, 'fight': True, 'pandemic': Tru...</td>\n",
       "      <td>0</td>\n",
       "      <td>we are still fighting a pandemic. and covid-19...</td>\n",
       "      <td>we are still fighting a pandemic and covid19 i...</td>\n",
       "      <td>still fighting pandemic fighting back ðŸš¨ practi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>{'everyone': True, 'flu': True, 'shot': True, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>everyone that had a flu shot will test positiv...</td>\n",
       "      <td>everyone that had a flu shot will test positiv...</td>\n",
       "      <td>everyone flu shot test positive virusjust sayi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>{'immunity': True, 'important': True, 'fight':...</td>\n",
       "      <td>0</td>\n",
       "      <td>immunity is important in the fight against cor...</td>\n",
       "      <td>immunity is important in the fight against cor...</td>\n",
       "      <td>immunity important fight against ayurveda comp...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tokens  Label  \\\n",
       "15   {'two': True, 'oxford': True, 'professor': Tru...      1   \n",
       "169  {'read': True, 'fake': True, 'news': True, 'wh...      1   \n",
       "133  {'still': True, 'fight': True, 'pandemic': Tru...      0   \n",
       "168  {'everyone': True, 'flu': True, 'shot': True, ...      1   \n",
       "131  {'immunity': True, 'important': True, 'fight':...      0   \n",
       "\n",
       "                                                  Text  \\\n",
       "15   two oxford professors and one at dundee have s...   \n",
       "169  read that some fake news whatsapp forward talk...   \n",
       "133  we are still fighting a pandemic. and covid-19...   \n",
       "168  everyone that had a flu shot will test positiv...   \n",
       "131  immunity is important in the fight against cor...   \n",
       "\n",
       "                                            Text_Clean  \\\n",
       "15   two oxford professors and one at dundee have s...   \n",
       "169  read that some fake news whatsapp forward talk...   \n",
       "133  we are still fighting a pandemic and covid19 i...   \n",
       "168  everyone that had a flu shot will test positiv...   \n",
       "131  immunity is important in the fight against cor...   \n",
       "\n",
       "                                            Text_Final  true_label  \\\n",
       "15   two oxford professors one dundee scientific ev...           0   \n",
       "169  read fake news whatsapp forward talks drinking...           0   \n",
       "133  still fighting pandemic fighting back ðŸš¨ practi...           1   \n",
       "168  everyone flu shot test positive virusjust sayi...           0   \n",
       "131  immunity important fight against ayurveda comp...           1   \n",
       "\n",
       "     misinfo_label  \n",
       "15               1  \n",
       "169              1  \n",
       "133              0  \n",
       "168              1  \n",
       "131              0  "
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_tweets_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports for CNN/ LSTM parts\n",
    "from __future__ import division, print_function\n",
    "from gensim import models\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing for true / misinfo label creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_tweets_data['Label'] = merge_tweets_data['Label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "true_label = []\n",
    "misinfo_label = []\n",
    "for l in merge_tweets_data.Label:\n",
    "    if l == 0:\n",
    "        true_label.append(1)\n",
    "        misinfo_label.append(0)\n",
    "    elif l == 1:\n",
    "        true_label.append(0)\n",
    "        misinfo_label.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merge_tweets_data['true_label']= true_label\n",
    "merge_tweets_data['misinfo_label']= misinfo_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "tokens = [word_tokenize(sen) for sen in merge_tweets_data.Text_Clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_token(tokens): \n",
    "    return [w.lower() for w in tokens]    \n",
    "    \n",
    "lower_tokens = [lower_token(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_stop_words(tokens): \n",
    "    return [word for word in tokens if word not in our_stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk import word_tokenize, WordNetLemmatizer\n",
    "#tokens = [word_tokenize(sen) for sen in merge_tweets_data.Text_Clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = [remove_stop_words(sen) for sen in lower_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [' '.join(sen) for sen in filtered_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-222-83b2d4933e1c>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merge_tweets_data['Text_Final'] = result\n"
     ]
    }
   ],
   "source": [
    "merge_tweets_data['Text_Final'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Testing split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(merge_tweets_data, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5376 words total, with a vocabulary size of 2289\n",
      "Max sentence length is 33\n"
     ]
    }
   ],
   "source": [
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1341 words total, with a vocabulary size of 820\n",
      "Max sentence length is 28\n"
     ]
    }
   ],
   "source": [
    "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenizing and padding sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2580 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train[\"Text_Final\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(data_train[\"Text_Final\"].tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2581, 300)\n"
     ]
    }
   ],
   "source": [
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(data_test[\"Text_Final\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "\n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge_tweets_data['true_label'] = merge_tweets_data['true_label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge_tweets_data['misinfo_label'] = merge_tweets_data['misinfo_label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['true_label', 'misinfo_label'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_train[label_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = train_cnn_data\n",
    "y_tr = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 50, 300)      774300      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 49, 200)      120200      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 48, 200)      180200      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 47, 200)      240200      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 46, 200)      300200      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 45, 200)      360200      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 200)          0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 200)          0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 200)          0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 200)          0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 200)          0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1000)         0           global_max_pooling1d_5[0][0]     \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1000)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          128128      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 2)            258         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,103,686\n",
      "Trainable params: 1,329,386\n",
      "Non-trainable params: 774,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_cnn = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
    "                len(list(label_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "batch_size = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 0.7700 - acc: 0.5794 - val_loss: 0.6777 - val_acc: 0.6667\n",
      "Epoch 2/3\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.5718 - acc: 0.7249 - val_loss: 0.6634 - val_acc: 0.6667\n",
      "Epoch 3/3\n",
      "12/12 [==============================] - 0s 39ms/step - loss: 0.4606 - acc: 0.8228 - val_loss: 0.5845 - val_acc: 0.6667\n"
     ]
    }
   ],
   "source": [
    "hist_cnn = model_cnn.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 993us/step\n"
     ]
    }
   ],
   "source": [
    "predictions_cnn = model_cnn.predict(test_cnn_data, batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_labels=[]\n",
    "for p in predictions_cnn:\n",
    "    prediction_labels.append(labels[np.argmax(p)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.638095238095238"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data_test.Label==prediction_labels)/len(prediction_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    56\n",
       "1    49\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.Label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN/LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurrent_nn(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "#     lstm = LSTM(256, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(embedded_sequences)\n",
    "    lstm = LSTM(256)(embedded_sequences)\n",
    "    \n",
    "    x = Dense(128, activation='relu')(lstm)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 50, 300)           774300    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               570368    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 1,377,822\n",
      "Trainable params: 603,522\n",
      "Non-trainable params: 774,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_rnn = recurrent_nn(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
    "                len(list(label_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 5\n",
    "batch_size = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "12/12 [==============================] - 1s 122ms/step - loss: 0.6687 - acc: 0.5899 - val_loss: 0.6743 - val_acc: 0.7143\n",
      "Epoch 2/5\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.6558 - acc: 0.6402 - val_loss: 0.6628 - val_acc: 0.5952\n",
      "Epoch 3/5\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 0.5966 - acc: 0.7196 - val_loss: 0.7178 - val_acc: 0.6190\n",
      "Epoch 4/5\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 0.5744 - acc: 0.7275 - val_loss: 0.6281 - val_acc: 0.6667\n",
      "Epoch 5/5\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.5057 - acc: 0.7381 - val_loss: 0.5644 - val_acc: 0.7857\n"
     ]
    }
   ],
   "source": [
    "hist = model_rnn.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions_lstm = model_rnn.predict(test_cnn_data, batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_labels=[]\n",
    "for p in predictions_lstm:\n",
    "    prediction_labels.append(labels[np.argmax(p)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6476190476190476"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sum(data_test.Label==prediction_labels)/len(prediction_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Earlier for training testing with NLTK classifiers (ML classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'scientist': True,\n",
       "  'medical': True,\n",
       "  'researcher': True,\n",
       "  'work': True,\n",
       "  'find': True,\n",
       "  'vaccine': True,\n",
       "  'against': True,\n",
       "  'virus': True,\n",
       "  'until': True,\n",
       "  'stop': True},\n",
       " '0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_tweet_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(true_tweet_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = Merge_dataset[:421]\n",
    "test_data = Merge_dataset[422:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.DataFrame(train_data, columns=['Text', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'greaternoidawest': True, 'coronavirusindia':...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'worldwide': True, 'pandemic': True, 'cause':...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'follow': True, 'social': True, 'distancing':...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'roque': True, 'implementation': True, 'lockd...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Label\n",
       "0  {'greaternoidawest': True, 'coronavirusindia':...     0\n",
       "1  {'worldwide': True, 'pandemic': True, 'cause':...     0\n",
       "2  {'follow': True, 'social': True, 'distancing':...     0\n",
       "3  {'roque': True, 'implementation': True, 'lockd...     0"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>{'37': True, 'virus-infected': True, 'patient'...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>{'one': True, 'nova': True, 'health': True, 'c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>{'pandemic': True, 'teach': True, 'everyone': ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>{'wear': True, 'mask': True, 'please': True}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>{'two': True, 'oxford': True, 'professor': Tru...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'number': True, 'japanese': True, 'antibody':...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>{'case': True, 'rise': True, 'rapidly': True, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>{'social': True, 'distance': True, 'very': Tru...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'hydroxychloroquin': True, 'approve': True, '...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>{'fda': True, 'pull': True, 'emergency': True,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>421 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text Label\n",
       "329  {'37': True, 'virus-infected': True, 'patient'...     0\n",
       "179  {'one': True, 'nova': True, 'health': True, 'c...     0\n",
       "174  {'pandemic': True, 'teach': True, 'everyone': ...     0\n",
       "402       {'wear': True, 'mask': True, 'please': True}     0\n",
       "286  {'two': True, 'oxford': True, 'professor': Tru...     1\n",
       "..                                                 ...   ...\n",
       "17   {'number': True, 'japanese': True, 'antibody':...     0\n",
       "178  {'case': True, 'rise': True, 'rapidly': True, ...     0\n",
       "189  {'social': True, 'distance': True, 'very': Tru...     0\n",
       "4    {'hydroxychloroquin': True, 'approve': True, '...     1\n",
       "65   {'fda': True, 'pull': True, 'emergency': True,...     1\n",
       "\n",
       "[421 rows x 2 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.utils.shuffle(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df = pd.DataFrame(test_data, columns=['Text', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'support': True, 'lockdown': True, 'stop': Tr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'early': True, 'detection': True, 'monitoring...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'federal': True, 'law': True, 'allow': True, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'ðŸ˜·': True, 'friend': True, 'buy': True, 'mask...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Label\n",
       "0  {'support': True, 'lockdown': True, 'stop': Tr...     0\n",
       "1  {'early': True, 'detection': True, 'monitoring...     0\n",
       "2  {'federal': True, 'law': True, 'allow': True, ...     1\n",
       "3  {'ðŸ˜·': True, 'friend': True, 'buy': True, 'mask...     0"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 2)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(421, 2)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data = pd.concat([testing_df, training_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(524, 2)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'support': True, 'lockdown': True, 'stop': Tr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'early': True, 'detection': True, 'monitoring...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'federal': True, 'law': True, 'allow': True, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'ðŸ˜·': True, 'friend': True, 'buy': True, 'mask...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'stay': True, 'home': True, 'unique': True, '...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Label\n",
       "0  {'support': True, 'lockdown': True, 'stop': Tr...     0\n",
       "1  {'early': True, 'detection': True, 'monitoring...     0\n",
       "2  {'federal': True, 'law': True, 'allow': True, ...     1\n",
       "3  {'ðŸ˜·': True, 'friend': True, 'buy': True, 'mask...     0\n",
       "4  {'stay': True, 'home': True, 'unique': True, '...     0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '1'], dtype=object)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_data.Label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = []\n",
    "misinfo = []\n",
    "for l in merge_data.Label:\n",
    "    if l == '0':\n",
    "        #print(\"it went in if\")\n",
    "        true.append(1)\n",
    "        misinfo.append(0)\n",
    "    elif l == '1':\n",
    "        true.append(0)\n",
    "        misinfo.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(misinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>tru</th>\n",
       "      <th>mis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>{'scientist': True, 'face': True, 'mask': True...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>{'view': True, 'increase': True, 'status': Tru...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'woman': True, '15': True, 'friend': True, 't...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>{'bayer': True, 'offer': True, 'anti': True, '...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>{'friend': True, 'india': True, 'win': True, '...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>{'lockdown': True, 'give': True, 'high': True,...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'support': True, 'lockdown': True, 'stop': Tr...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>{'korean': True, 'study': True, 'sars-cov-2': ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'even': True, 'need': True, 'hydroxychlorquin...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>{'good': True, 'long': True, 'stay': True, 'ho...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>524 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text Label  tru  mis\n",
       "21   {'scientist': True, 'face': True, 'mask': True...     1    0    1\n",
       "293  {'view': True, 'increase': True, 'status': Tru...     0    1    0\n",
       "10   {'woman': True, '15': True, 'friend': True, 't...     0    1    0\n",
       "400  {'bayer': True, 'offer': True, 'anti': True, '...     1    0    1\n",
       "85   {'friend': True, 'india': True, 'win': True, '...     1    0    1\n",
       "..                                                 ...   ...  ...  ...\n",
       "367  {'lockdown': True, 'give': True, 'high': True,...     0    1    0\n",
       "0    {'support': True, 'lockdown': True, 'stop': Tr...     0    1    0\n",
       "336  {'korean': True, 'study': True, 'sars-cov-2': ...     0    1    0\n",
       "17   {'even': True, 'need': True, 'hydroxychlorquin...     1    0    1\n",
       "78   {'good': True, 'long': True, 'stay': True, 'ho...     0    1    0\n",
       "\n",
       "[524 rows x 4 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.utils.shuffle(merge_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data['tru']= true\n",
    "merge_data['mis']= misinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>tru</th>\n",
       "      <th>mis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'support': True, 'lockdown': True, 'stop': Tr...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'early': True, 'detection': True, 'monitoring...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'federal': True, 'law': True, 'allow': True, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'ðŸ˜·': True, 'friend': True, 'buy': True, 'mask...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'stay': True, 'home': True, 'unique': True, '...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Label  tru  mis\n",
       "0  {'support': True, 'lockdown': True, 'stop': Tr...     0    1    0\n",
       "1  {'early': True, 'detection': True, 'monitoring...     0    1    0\n",
       "2  {'federal': True, 'law': True, 'allow': True, ...     1    0    1\n",
       "3  {'ðŸ˜·': True, 'friend': True, 'buy': True, 'mask...     0    1    0\n",
       "4  {'stay': True, 'home': True, 'unique': True, '...     0    1    0"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments['Text'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(merge_data, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435 words total, with a vocabulary size of 2359\n",
      "Max sentence length is 33\n"
     ]
    }
   ],
   "source": [
    "all_training_words = [word for tokens in data_train[\"Text\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"Text\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1273 words total, with a vocabulary size of 772\n",
      "Max sentence length is 30\n"
     ]
    }
   ],
   "source": [
    "all_test_words = [word for tokens in data_test[\"Text\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test[\"Text\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7:Classifiers with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk import DecisionTreeClassifier\n",
    "from nltk import MaxentClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2 = DecisionTreeClassifier.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.404\n",
      "             2          -0.43919        0.929\n",
      "             3          -0.33655        0.955\n",
      "             4          -0.27652        0.971\n",
      "             5          -0.23600        0.974\n",
      "             6          -0.20643        0.988\n",
      "             7          -0.18375        0.990\n",
      "             8          -0.16574        0.993\n",
      "             9          -0.15106        0.993\n",
      "            10          -0.13884        0.993\n",
      "            11          -0.12851        0.995\n",
      "            12          -0.11965        0.995\n",
      "            13          -0.11197        0.995\n",
      "            14          -0.10524        0.995\n",
      "            15          -0.09930        0.995\n",
      "            16          -0.09402        0.995\n",
      "            17          -0.08928        0.995\n",
      "            18          -0.08502        0.995\n",
      "            19          -0.08115        0.995\n",
      "            20          -0.07764        0.995\n",
      "            21          -0.07442        0.995\n",
      "            22          -0.07147        0.995\n",
      "            23          -0.06876        0.995\n",
      "            24          -0.06625        0.995\n",
      "            25          -0.06393        0.995\n",
      "            26          -0.06176        0.995\n",
      "            27          -0.05975        0.995\n",
      "            28          -0.05787        0.995\n",
      "            29          -0.05611        0.995\n",
      "            30          -0.05445        0.995\n",
      "            31          -0.05290        0.995\n",
      "            32          -0.05143        0.998\n",
      "            33          -0.05005        0.998\n",
      "            34          -0.04875        0.998\n",
      "            35          -0.04751        0.998\n",
      "            36          -0.04634        0.998\n",
      "            37          -0.04522        0.998\n",
      "            38          -0.04416        0.998\n",
      "            39          -0.04315        0.998\n",
      "            40          -0.04219        0.998\n",
      "            41          -0.04128        0.998\n",
      "            42          -0.04040        0.998\n",
      "            43          -0.03956        0.998\n",
      "            44          -0.03876        0.998\n",
      "            45          -0.03799        0.998\n",
      "            46          -0.03725        0.998\n",
      "            47          -0.03654        0.998\n",
      "            48          -0.03586        0.998\n",
      "            49          -0.03521        0.998\n",
      "            50          -0.03458        0.998\n",
      "            51          -0.03397        0.998\n",
      "            52          -0.03338        0.998\n",
      "            53          -0.03282        0.998\n",
      "            54          -0.03227        0.998\n",
      "            55          -0.03175        0.998\n",
      "            56          -0.03124        0.998\n",
      "            57          -0.03075        0.998\n",
      "            58          -0.03027        0.998\n",
      "            59          -0.02981        0.998\n",
      "            60          -0.02936        0.998\n",
      "            61          -0.02893        0.998\n",
      "            62          -0.02851        0.998\n",
      "            63          -0.02810        0.998\n",
      "            64          -0.02771        0.998\n",
      "            65          -0.02732        0.998\n",
      "            66          -0.02695        0.998\n",
      "            67          -0.02659        0.998\n",
      "            68          -0.02623        0.998\n",
      "            69          -0.02589        0.998\n",
      "            70          -0.02556        0.998\n",
      "            71          -0.02523        0.998\n",
      "            72          -0.02491        0.998\n",
      "            73          -0.02461        0.998\n",
      "            74          -0.02431        0.998\n",
      "            75          -0.02401        0.998\n",
      "            76          -0.02373        0.998\n",
      "            77          -0.02345        0.998\n",
      "            78          -0.02317        0.998\n",
      "            79          -0.02291        0.998\n",
      "            80          -0.02265        0.998\n",
      "            81          -0.02239        0.998\n",
      "            82          -0.02214        0.998\n",
      "            83          -0.02190        0.998\n",
      "            84          -0.02166        0.998\n",
      "            85          -0.02143        0.998\n",
      "            86          -0.02121        0.998\n",
      "            87          -0.02098        1.000\n",
      "            88          -0.02077        1.000\n",
      "            89          -0.02055        1.000\n",
      "            90          -0.02034        1.000\n",
      "            91          -0.02014        1.000\n",
      "            92          -0.01994        1.000\n",
      "            93          -0.01974        1.000\n",
      "            94          -0.01955        1.000\n",
      "            95          -0.01936        1.000\n",
      "            96          -0.01918        1.000\n",
      "            97          -0.01900        1.000\n",
      "            98          -0.01882        1.000\n",
      "            99          -0.01864        1.000\n",
      "         Final          -0.01847        1.000\n"
     ]
    }
   ],
   "source": [
    "classifier_mxnet = MaxentClassifier.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 73.7864077669903\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 67.96116504854369\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy is:\", classify.accuracy(classifier2, test_data)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 68.93203883495146\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy is:\", classify.accuracy(classifier_mxnet, test_data)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics.scores import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for calculating precision adn recall creating refset and testsets\n",
    "refsets = collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "for i, (feats, label) in enumerate(test_data):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True info Precision: 0.7966101694915254\n",
      "Misinfo Precision: 0.6590909090909091\n",
      "True info Recall: 0.7580645161290323\n",
      "Misinfo Recall: 0.7073170731707317\n",
      "True info F-measure: 0.7768595041322315\n",
      "Misinfo F-measure: 0.6823529411764705\n"
     ]
    }
   ],
   "source": [
    "# metrics for NB classifiers (bag of words)\n",
    "print( 'True info Precision:', precision(refsets['0'], testsets['0']))\n",
    "print( 'Misinfo Precision:', precision(refsets['1'], testsets['1']))\n",
    "print( 'True info Recall:', recall(refsets['0'], testsets['0']))\n",
    "print( 'Misinfo Recall:', recall(refsets['1'], testsets['1']) )\n",
    "print( 'True info F-measure:', f_measure(refsets['0'], testsets['0']))\n",
    "print( 'Misinfo F-measure:', f_measure(refsets['1'], testsets['1']) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for calculating precision adn recall creating refset and testsets\n",
    "refsets = collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "for i, (feats, label) in enumerate(test_data):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier2.classify(feats)\n",
    "    testsets[observed].add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True info Precision: 0.7301587301587301\n",
      "Misinfo Precision: 0.6\n",
      "True info Recall: 0.7419354838709677\n",
      "Misinfo Recall: 0.5853658536585366\n",
      "True info F-measure: 0.736\n",
      "Misinfo F-measure: 0.5925925925925926\n"
     ]
    }
   ],
   "source": [
    "# metrics for NB classifiers (bag of words)\n",
    "print( 'True info Precision:', precision(refsets['0'], testsets['0']))\n",
    "print( 'Misinfo Precision:', precision(refsets['1'], testsets['1']))\n",
    "print( 'True info Recall:', recall(refsets['0'], testsets['0']))\n",
    "print( 'Misinfo Recall:', recall(refsets['1'], testsets['1']) )\n",
    "print( 'True info F-measure:', f_measure(refsets['0'], testsets['0']))\n",
    "print( 'Misinfo F-measure:', f_measure(refsets['1'], testsets['1']) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for calculating precision adn recall creating refset and testsets\n",
    "refsets = collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "for i, (feats, label) in enumerate(test_data):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier_mxnet.classify(feats)\n",
    "    testsets[observed].add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True info Precision: 0.7142857142857143\n",
      "Misinfo Precision: 0.6363636363636364\n",
      "True info Recall: 0.8064516129032258\n",
      "Misinfo Recall: 0.5121951219512195\n",
      "True info F-measure: 0.7575757575757577\n",
      "Misinfo F-measure: 0.5675675675675675\n"
     ]
    }
   ],
   "source": [
    "# metrics for NB classifiers (bag of words)\n",
    "print( 'True info Precision:', precision(refsets['0'], testsets['0']))\n",
    "print( 'Misinfo Precision:', precision(refsets['1'], testsets['1']))\n",
    "print( 'True info Recall:', recall(refsets['0'], testsets['0']))\n",
    "print( 'Misinfo Recall:', recall(refsets['1'], testsets['1']) )\n",
    "print( 'True info F-measure:', f_measure(refsets['0'], testsets['0']))\n",
    "print( 'Misinfo F-measure:', f_measure(refsets['1'], testsets['1']) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_svm = SklearnClassifier(SVC()).train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 68.93203883495146\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy is:\", classify.accuracy(classifier_svm, test_data)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_rf = SklearnClassifier(RandomForestClassifier()).train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 67.96116504854369\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy is:\", classify.accuracy(classifier_rf, test_data)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for calculating precision adn recall creating refset and testsets\n",
    "refsets = collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "for i, (feats, label) in enumerate(test_data):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier_svm.classify(feats)\n",
    "    testsets[observed].add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True info Precision: 0.6785714285714286\n",
      "Misinfo Precision: 0.7368421052631579\n",
      "True info Recall: 0.9193548387096774\n",
      "Misinfo Recall: 0.34146341463414637\n",
      "True info F-measure: 0.7808219178082192\n",
      "Misinfo F-measure: 0.4666666666666667\n"
     ]
    }
   ],
   "source": [
    "# metrics for NB classifiers (bag of words)\n",
    "print( 'True info Precision:', precision(refsets['0'], testsets['0']))\n",
    "print( 'Misinfo Precision:', precision(refsets['1'], testsets['1']))\n",
    "print( 'True info Recall:', recall(refsets['0'], testsets['0']))\n",
    "print( 'Misinfo Recall:', recall(refsets['1'], testsets['1']) )\n",
    "print( 'True info F-measure:', f_measure(refsets['0'], testsets['0']))\n",
    "print( 'Misinfo F-measure:', f_measure(refsets['1'], testsets['1']) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for calculating precision adn recall creating refset and testsets\n",
    "refsets = collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "for i, (feats, label) in enumerate(test_data):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier_rf.classify(feats)\n",
    "    testsets[observed].add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True info Precision: 0.6746987951807228\n",
      "Misinfo Precision: 0.7\n",
      "True info Recall: 0.9032258064516129\n",
      "Misinfo Recall: 0.34146341463414637\n",
      "True info F-measure: 0.7724137931034483\n",
      "Misinfo F-measure: 0.459016393442623\n"
     ]
    }
   ],
   "source": [
    "# metrics for NB classifiers (bag of words)\n",
    "print( 'True info Precision:', precision(refsets['0'], testsets['0']))\n",
    "print( 'Misinfo Precision:', precision(refsets['1'], testsets['1']))\n",
    "print( 'True info Recall:', recall(refsets['0'], testsets['0']))\n",
    "print( 'Misinfo Recall:', recall(refsets['1'], testsets['1']) )\n",
    "print( 'True info F-measure:', f_measure(refsets['0'], testsets['0']))\n",
    "print( 'Misinfo F-measure:', f_measure(refsets['1'], testsets['1']) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "      hydroxychloroquine = True                1 : 0      =     13.6 : 1.0\n",
      "                   right = True                1 : 0      =     13.0 : 1.0\n",
      "                     fda = True                1 : 0      =     12.0 : 1.0\n",
      "                      5g = True                1 : 0      =     11.1 : 1.0\n",
      "                     few = True                1 : 0      =     11.1 : 1.0\n",
      "                   treat = True                1 : 0      =     10.1 : 1.0\n",
      "                    home = True                0 : 1      =      9.0 : 1.0\n",
      "           authorization = True                1 : 0      =      8.2 : 1.0\n",
      "                    safe = True                0 : 1      =      8.1 : 1.0\n",
      "               emergency = True                1 : 0      =      5.6 : 1.0\n",
      "                   india = True                0 : 1      =      5.3 : 1.0\n",
      "                actually = True                1 : 0      =      5.3 : 1.0\n",
      "                   drink = True                1 : 0      =      5.3 : 1.0\n",
      "                  during = True                1 : 0      =      5.3 : 1.0\n",
      "                   trump = True                1 : 0      =      4.6 : 1.0\n",
      "                lockdown = True                0 : 1      =      4.5 : 1.0\n",
      "                   doesn = True                1 : 0      =      4.3 : 1.0\n",
      "                  update = True                1 : 0      =      4.3 : 1.0\n",
      "                    wave = True                1 : 0      =      4.3 : 1.0\n",
      "                     yes = True                1 : 0      =      4.3 : 1.0\n",
      "                   break = True                0 : 1      =      3.9 : 1.0\n",
      "                 disease = True                0 : 1      =      3.9 : 1.0\n",
      "                  family = True                0 : 1      =      3.9 : 1.0\n",
      "                  please = True                0 : 1      =      3.9 : 1.0\n",
      "                    down = True                1 : 0      =      3.8 : 1.0\n",
      "                medicine = True                1 : 0      =      3.8 : 1.0\n",
      "                    back = True                0 : 1      =      3.5 : 1.0\n",
      "                 contact = True                0 : 1      =      3.5 : 1.0\n",
      "                stayhome = True                0 : 1      =      3.5 : 1.0\n",
      "                    stay = True                0 : 1      =      3.4 : 1.0\n",
      "                      ca = True                1 : 0      =      3.4 : 1.0\n",
      "                     hot = True                1 : 0      =      3.4 : 1.0\n",
      "                 malaria = True                1 : 0      =      3.4 : 1.0\n",
      "                    mind = True                1 : 0      =      3.4 : 1.0\n",
      "                  number = True                1 : 0      =      3.4 : 1.0\n",
      "                  oxford = True                1 : 0      =      3.4 : 1.0\n",
      "                  report = True                1 : 0      =      3.4 : 1.0\n",
      "                  second = True                1 : 0      =      3.4 : 1.0\n",
      "                   those = True                1 : 0      =      3.4 : 1.0\n",
      "                withdraw = True                1 : 0      =      3.4 : 1.0\n",
      "                   wuhan = True                1 : 0      =      3.4 : 1.0\n",
      "                   child = True                1 : 0      =      3.2 : 1.0\n",
      "                  social = True                0 : 1      =      3.2 : 1.0\n",
      "                    drug = True                1 : 0      =      3.1 : 1.0\n",
      "                  doctor = True                1 : 0      =      3.1 : 1.0\n",
      "                  spread = True                0 : 1      =      3.1 : 1.0\n",
      "                    long = True                0 : 1      =      3.0 : 1.0\n",
      "                 protect = True                0 : 1      =      3.0 : 1.0\n",
      "                    risk = True                0 : 1      =      3.0 : 1.0\n",
      "                    hand = True                0 : 1      =      2.9 : 1.0\n",
      "                    need = True                1 : 0      =      2.7 : 1.0\n",
      "               treatment = True                1 : 0      =      2.7 : 1.0\n",
      "                    hoax = True                1 : 0      =      2.7 : 1.0\n",
      "                   study = True                1 : 0      =      2.7 : 1.0\n",
      "                    tell = True                1 : 0      =      2.7 : 1.0\n",
      "                    bill = True                1 : 0      =      2.6 : 1.0\n",
      "                    show = True                1 : 0      =      2.6 : 1.0\n",
      "                     now = True                1 : 0      =      2.6 : 1.0\n",
      "                    care = True                0 : 1      =      2.5 : 1.0\n",
      "                     end = True                0 : 1      =      2.5 : 1.0\n",
      "                    hard = True                0 : 1      =      2.5 : 1.0\n",
      "                hospital = True                0 : 1      =      2.5 : 1.0\n",
      "                 medical = True                0 : 1      =      2.5 : 1.0\n",
      "                 serious = True                0 : 1      =      2.5 : 1.0\n",
      "                 suggest = True                0 : 1      =      2.5 : 1.0\n",
      "                immunity = True                1 : 0      =      2.5 : 1.0\n",
      "                      up = True                1 : 0      =      2.4 : 1.0\n",
      "                   allow = True                1 : 0      =      2.4 : 1.0\n",
      "                     bad = True                1 : 0      =      2.4 : 1.0\n",
      "                     bat = True                1 : 0      =      2.4 : 1.0\n",
      "                     big = True                1 : 0      =      2.4 : 1.0\n",
      "                breaking = True                1 : 0      =      2.4 : 1.0\n",
      "                    burn = True                1 : 0      =      2.4 : 1.0\n",
      "                   cause = True                1 : 0      =      2.4 : 1.0\n",
      "                    city = True                1 : 0      =      2.4 : 1.0\n",
      "                cocktail = True                1 : 0      =      2.4 : 1.0\n",
      "                 combine = True                1 : 0      =      2.4 : 1.0\n",
      "                 contain = True                1 : 0      =      2.4 : 1.0\n",
      "                  course = True                1 : 0      =      2.4 : 1.0\n",
      "                  create = True                1 : 0      =      2.4 : 1.0\n",
      "                  depend = True                1 : 0      =      2.4 : 1.0\n",
      "                  expert = True                1 : 0      =      2.4 : 1.0\n",
      "                   first = True                1 : 0      =      2.4 : 1.0\n",
      "                    herd = True                1 : 0      =      2.4 : 1.0\n",
      "             immediately = True                1 : 0      =      2.4 : 1.0\n",
      "                 include = True                1 : 0      =      2.4 : 1.0\n",
      "                     law = True                1 : 0      =      2.4 : 1.0\n",
      "                   light = True                1 : 0      =      2.4 : 1.0\n",
      "                    list = True                1 : 0      =      2.4 : 1.0\n",
      "                     lot = True                1 : 0      =      2.4 : 1.0\n",
      "                    near = True                1 : 0      =      2.4 : 1.0\n",
      "                  period = True                1 : 0      =      2.4 : 1.0\n",
      "                    post = True                1 : 0      =      2.4 : 1.0\n",
      "                 require = True                1 : 0      =      2.4 : 1.0\n",
      "                  result = True                1 : 0      =      2.4 : 1.0\n",
      "                  safely = True                1 : 0      =      2.4 : 1.0\n",
      "                    sars = True                1 : 0      =      2.4 : 1.0\n",
      "                   texas = True                1 : 0      =      2.4 : 1.0\n",
      "                     try = True                1 : 0      =      2.4 : 1.0\n",
      "                    zero = True                1 : 0      =      2.4 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(classifier.show_most_informative_features(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngrams methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngram_features(words, n=2):\n",
    "    ngram_vocab = ngrams(words, n)\n",
    "    my_dict = dict([(ng, True) for ng in ngram_vocab])\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram accuracy: 0.735\n",
      "2-gram accuracy: 0.7625\n",
      "3-gram accuracy: 0.8275\n",
      "4-gram accuracy: 0.8125\n",
      "5-gram accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "for n in [1,2,3,4,5]:\n",
    "    pos_data = []\n",
    "    for fileid in movie_reviews.fileids('pos'):\n",
    "        words = movie_reviews.words(fileid)\n",
    "        pos_data.append((create_ngram_features(words, n), \"positive\"))    \n",
    "\n",
    "    neg_data = []\n",
    "    for fileid in movie_reviews.fileids('neg'):\n",
    "        words = movie_reviews.words(fileid)\n",
    "        neg_data.append((create_ngram_features(words, n), \"negative\")) \n",
    "\n",
    "    train_set = pos_data[:800] + neg_data[:800]\n",
    "    test_set =  pos_data[800:] + neg_data[800:]\n",
    "\n",
    "    classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    accuracy = nltk.classify.util.accuracy(classifier, test_set)\n",
    "    print(str(n)+'-gram accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_tweet_dataset[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram size of total data  450\n",
      "1-gram size of training data  360\n",
      "1-gram size of test data  90\n",
      "1-gram accuracy for NB: 0.6333333333333333\n",
      "True info Precision: 0.6779661016949152\n",
      "Misinfo Precision: 0.5483870967741935\n",
      "True info Recall: 0.7407407407407407\n",
      "Misinfo Recall: 0.4722222222222222\n",
      "True info F-measure: 0.7079646017699115\n",
      "Misinfo F-measure: 0.5074626865671642\n",
      "1-gram accuracy for DT: 0.6555555555555556\n",
      "True info Precision: 0.7017543859649122\n",
      "Misinfo Precision: 0.5757575757575758\n",
      "True info Recall: 0.7407407407407407\n",
      "Misinfo Recall: 0.5277777777777778\n",
      "True info F-measure: 0.7207207207207206\n",
      "Misinfo F-measure: 0.5507246376811594\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.456\n",
      "             2          -0.43368        0.939\n",
      "             3          -0.33140        0.961\n",
      "             4          -0.27230        0.972\n",
      "             5          -0.23250        0.981\n",
      "             6          -0.20346        0.989\n",
      "             7          -0.18116        0.992\n",
      "             8          -0.16345        0.992\n",
      "             9          -0.14901        0.994\n",
      "            10          -0.13700        0.994\n",
      "            11          -0.12684        0.994\n",
      "            12          -0.11814        0.994\n",
      "            13          -0.11060        0.994\n",
      "            14          -0.10400        0.994\n",
      "            15          -0.09817        0.994\n",
      "            16          -0.09299        0.994\n",
      "            17          -0.08835        0.994\n",
      "            18          -0.08417        0.994\n",
      "            19          -0.08039        0.994\n",
      "            20          -0.07695        0.994\n",
      "            21          -0.07381        0.994\n",
      "            22          -0.07093        0.994\n",
      "            23          -0.06827        0.994\n",
      "            24          -0.06582        0.994\n",
      "            25          -0.06355        0.994\n",
      "            26          -0.06144        0.994\n",
      "            27          -0.05948        0.994\n",
      "            28          -0.05764        0.994\n",
      "            29          -0.05593        0.994\n",
      "            30          -0.05431        0.994\n",
      "            31          -0.05280        0.997\n",
      "            32          -0.05137        0.997\n",
      "            33          -0.05003        0.997\n",
      "            34          -0.04875        0.997\n",
      "            35          -0.04755        0.997\n",
      "            36          -0.04641        0.997\n",
      "            37          -0.04532        0.997\n",
      "            38          -0.04429        0.997\n",
      "            39          -0.04331        0.997\n",
      "            40          -0.04238        0.997\n",
      "            41          -0.04148        0.997\n",
      "            42          -0.04063        0.997\n",
      "            43          -0.03981        0.997\n",
      "            44          -0.03903        0.997\n",
      "            45          -0.03828        0.997\n",
      "            46          -0.03757        0.997\n",
      "            47          -0.03688        0.997\n",
      "            48          -0.03621        0.997\n",
      "            49          -0.03558        0.997\n",
      "            50          -0.03496        0.997\n",
      "            51          -0.03437        0.997\n",
      "            52          -0.03380        0.997\n",
      "            53          -0.03325        0.997\n",
      "            54          -0.03272        0.997\n",
      "            55          -0.03221        0.997\n",
      "            56          -0.03171        0.997\n",
      "            57          -0.03123        0.997\n",
      "            58          -0.03077        0.997\n",
      "            59          -0.03032        0.997\n",
      "            60          -0.02988        0.997\n",
      "            61          -0.02946        0.997\n",
      "            62          -0.02905        0.997\n",
      "            63          -0.02865        0.997\n",
      "            64          -0.02826        0.997\n",
      "            65          -0.02789        0.997\n",
      "            66          -0.02752        0.997\n",
      "            67          -0.02717        0.997\n",
      "            68          -0.02682        0.997\n",
      "            69          -0.02649        0.997\n",
      "            70          -0.02616        0.997\n",
      "            71          -0.02584        0.997\n",
      "            72          -0.02553        0.997\n",
      "            73          -0.02523        0.997\n",
      "            74          -0.02493        0.997\n",
      "            75          -0.02464        0.997\n",
      "            76          -0.02436        0.997\n",
      "            77          -0.02409        0.997\n",
      "            78          -0.02382        0.997\n",
      "            79          -0.02356        0.997\n",
      "            80          -0.02330        0.997\n",
      "            81          -0.02305        0.997\n",
      "            82          -0.02280        0.997\n",
      "            83          -0.02256        0.997\n",
      "            84          -0.02233        0.997\n",
      "            85          -0.02210        0.997\n",
      "            86          -0.02188        0.997\n",
      "            87          -0.02166        0.997\n",
      "            88          -0.02144        0.997\n",
      "            89          -0.02123        0.997\n",
      "            90          -0.02102        0.997\n",
      "            91          -0.02082        0.997\n",
      "            92          -0.02062        0.997\n",
      "            93          -0.02043        0.997\n",
      "            94          -0.02024        0.997\n",
      "            95          -0.02005        0.997\n",
      "            96          -0.01986        0.997\n",
      "            97          -0.01968        0.997\n",
      "            98          -0.01951        0.997\n",
      "            99          -0.01933        0.997\n",
      "         Final          -0.01916        0.997\n",
      "1-gram accuracy for MXNet: 0.6666666666666666\n",
      "True info Precision: 0.6818181818181818\n",
      "Misinfo Precision: 0.625\n",
      "True info Recall: 0.8333333333333334\n",
      "Misinfo Recall: 0.4166666666666667\n",
      "True info F-measure: 0.7499999999999999\n",
      "Misinfo F-measure: 0.5\n",
      "1-gram accuracy for RF: 0.6555555555555556\n",
      "True info Precision: 0.6533333333333333\n",
      "Misinfo Precision: 0.6666666666666666\n",
      "True info Recall: 0.9074074074074074\n",
      "Misinfo Recall: 0.2777777777777778\n",
      "True info F-measure: 0.7596899224806202\n",
      "Misinfo F-measure: 0.3921568627450981\n",
      "1-gram accuracy for SVM: 0.6666666666666666\n",
      "True info Precision: 0.6714285714285714\n",
      "Misinfo Precision: 0.65\n",
      "True info Recall: 0.8703703703703703\n",
      "Misinfo Recall: 0.3611111111111111\n",
      "True info F-measure: 0.7580645161290321\n",
      "Misinfo F-measure: 0.4642857142857143\n",
      "Most Informative Features\n",
      "                     fda = True                1 : 0      =     13.3 : 1.0\n",
      "                     few = True                1 : 0      =     13.3 : 1.0\n",
      "                      5g = True                1 : 0      =     10.3 : 1.0\n",
      "               emergency = True                1 : 0      =      9.7 : 1.0\n",
      "                   treat = True                1 : 0      =      9.3 : 1.0\n",
      "      hydroxychloroquine = True                1 : 0      =      9.1 : 1.0\n",
      "                    home = True                0 : 1      =      8.4 : 1.0\n",
      "                   right = True                1 : 0      =      8.0 : 1.0\n",
      "                   child = True                1 : 0      =      7.4 : 1.0\n",
      "                   trump = True                1 : 0      =      7.0 : 1.0\n",
      "                    safe = True                0 : 1      =      6.6 : 1.0\n",
      "                 malaria = True                1 : 0      =      5.4 : 1.0\n",
      "               president = True                1 : 0      =      5.4 : 1.0\n",
      "                    drug = True                1 : 0      =      5.2 : 1.0\n",
      "                anything = True                1 : 0      =      4.4 : 1.0\n",
      "                    hoax = True                1 : 0      =      4.4 : 1.0\n",
      "                  report = True                1 : 0      =      4.4 : 1.0\n",
      "                  second = True                1 : 0      =      4.4 : 1.0\n",
      "                     yes = True                1 : 0      =      4.4 : 1.0\n",
      "                   india = True                0 : 1      =      4.3 : 1.0\n",
      "                      up = True                1 : 0      =      4.0 : 1.0\n",
      "                distance = True                0 : 1      =      3.8 : 1.0\n",
      "                actually = True                1 : 0      =      3.8 : 1.0\n",
      "                medicine = True                1 : 0      =      3.8 : 1.0\n",
      "                    stay = True                0 : 1      =      3.6 : 1.0\n",
      "                   agree = True                1 : 0      =      3.4 : 1.0\n",
      "                     bad = True                1 : 0      =      3.4 : 1.0\n",
      "                    best = True                1 : 0      =      3.4 : 1.0\n",
      "                   cause = True                1 : 0      =      3.4 : 1.0\n",
      "                    cell = True                1 : 0      =      3.4 : 1.0\n",
      "None\n",
      "2-gram size of total data  450\n",
      "2-gram size of training data  360\n",
      "2-gram size of test data  90\n",
      "2-gram accuracy for NB: 0.6222222222222222\n",
      "True info Precision: 0.5866666666666667\n",
      "Misinfo Precision: 0.8\n",
      "True info Recall: 0.9361702127659575\n",
      "Misinfo Recall: 0.27906976744186046\n",
      "True info F-measure: 0.7213114754098361\n",
      "Misinfo F-measure: 0.4137931034482758\n",
      "2-gram accuracy for DT: 0.5555555555555556\n",
      "True info Precision: 0.5421686746987951\n",
      "Misinfo Precision: 0.7142857142857143\n",
      "True info Recall: 0.9574468085106383\n",
      "Misinfo Recall: 0.11627906976744186\n",
      "True info F-measure: 0.6923076923076923\n",
      "Misinfo F-measure: 0.2\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.436\n",
      "             2          -0.36020        0.983\n",
      "             3          -0.26165        0.992\n",
      "             4          -0.20600        1.000\n",
      "             5          -0.17011        1.000\n",
      "             6          -0.14498        1.000\n",
      "             7          -0.12639        1.000\n",
      "             8          -0.11206        1.000\n",
      "             9          -0.10067        1.000\n",
      "            10          -0.09140        1.000\n",
      "            11          -0.08370        1.000\n",
      "            12          -0.07721        1.000\n",
      "            13          -0.07166        1.000\n",
      "            14          -0.06685        1.000\n",
      "            15          -0.06266        1.000\n",
      "            16          -0.05896        1.000\n",
      "            17          -0.05567        1.000\n",
      "            18          -0.05274        1.000\n",
      "            19          -0.05010        1.000\n",
      "            20          -0.04771        1.000\n",
      "            21          -0.04554        1.000\n",
      "            22          -0.04356        1.000\n",
      "            23          -0.04174        1.000\n",
      "            24          -0.04007        1.000\n",
      "            25          -0.03853        1.000\n",
      "            26          -0.03711        1.000\n",
      "            27          -0.03578        1.000\n",
      "            28          -0.03455        1.000\n",
      "            29          -0.03340        1.000\n",
      "            30          -0.03233        1.000\n",
      "            31          -0.03132        1.000\n",
      "            32          -0.03037        1.000\n",
      "            33          -0.02948        1.000\n",
      "            34          -0.02864        1.000\n",
      "            35          -0.02784        1.000\n",
      "            36          -0.02709        1.000\n",
      "            37          -0.02638        1.000\n",
      "            38          -0.02571        1.000\n",
      "            39          -0.02506        1.000\n",
      "            40          -0.02445        1.000\n",
      "            41          -0.02387        1.000\n",
      "            42          -0.02332        1.000\n",
      "            43          -0.02279        1.000\n",
      "            44          -0.02228        1.000\n",
      "            45          -0.02180        1.000\n",
      "            46          -0.02134        1.000\n",
      "            47          -0.02090        1.000\n",
      "            48          -0.02047        1.000\n",
      "            49          -0.02006        1.000\n",
      "            50          -0.01967        1.000\n",
      "            51          -0.01929        1.000\n",
      "            52          -0.01893        1.000\n",
      "            53          -0.01858        1.000\n",
      "            54          -0.01824        1.000\n",
      "            55          -0.01792        1.000\n",
      "            56          -0.01760        1.000\n",
      "            57          -0.01730        1.000\n",
      "            58          -0.01701        1.000\n",
      "            59          -0.01673        1.000\n",
      "            60          -0.01645        1.000\n",
      "            61          -0.01619        1.000\n",
      "            62          -0.01593        1.000\n",
      "            63          -0.01568        1.000\n",
      "            64          -0.01544        1.000\n",
      "            65          -0.01521        1.000\n",
      "            66          -0.01498        1.000\n",
      "            67          -0.01476        1.000\n",
      "            68          -0.01455        1.000\n",
      "            69          -0.01434        1.000\n",
      "            70          -0.01414        1.000\n",
      "            71          -0.01394        1.000\n",
      "            72          -0.01375        1.000\n",
      "            73          -0.01357        1.000\n",
      "            74          -0.01339        1.000\n",
      "            75          -0.01321        1.000\n",
      "            76          -0.01304        1.000\n",
      "            77          -0.01287        1.000\n",
      "            78          -0.01271        1.000\n",
      "            79          -0.01255        1.000\n",
      "            80          -0.01240        1.000\n",
      "            81          -0.01225        1.000\n",
      "            82          -0.01210        1.000\n",
      "            83          -0.01196        1.000\n",
      "            84          -0.01182        1.000\n",
      "            85          -0.01168        1.000\n",
      "            86          -0.01154        1.000\n",
      "            87          -0.01141        1.000\n",
      "            88          -0.01129        1.000\n",
      "            89          -0.01116        1.000\n",
      "            90          -0.01104        1.000\n",
      "            91          -0.01092        1.000\n",
      "            92          -0.01080        1.000\n",
      "            93          -0.01069        1.000\n",
      "            94          -0.01057        1.000\n",
      "            95          -0.01046        1.000\n",
      "            96          -0.01036        1.000\n",
      "            97          -0.01025        1.000\n",
      "            98          -0.01015        1.000\n",
      "            99          -0.01005        1.000\n",
      "         Final          -0.00995        1.000\n",
      "2-gram accuracy for MXNet: 0.6666666666666666\n",
      "True info Precision: 0.7073170731707317\n",
      "Misinfo Precision: 0.6326530612244898\n",
      "True info Recall: 0.6170212765957447\n",
      "Misinfo Recall: 0.7209302325581395\n",
      "True info F-measure: 0.6590909090909092\n",
      "Misinfo F-measure: 0.6739130434782609\n",
      "2-gram accuracy for RF: 0.5555555555555556\n",
      "True info Precision: 0.5402298850574713\n",
      "Misinfo Precision: 1.0\n",
      "True info Recall: 1.0\n",
      "Misinfo Recall: 0.06976744186046512\n",
      "True info F-measure: 0.7014925373134329\n",
      "Misinfo F-measure: 0.13043478260869565\n",
      "2-gram accuracy for SVM: 0.5555555555555556\n",
      "True info Precision: 0.5402298850574713\n",
      "Misinfo Precision: 1.0\n",
      "True info Recall: 1.0\n",
      "Misinfo Recall: 0.06976744186046512\n",
      "True info F-measure: 0.7014925373134329\n",
      "Misinfo F-measure: 0.13043478260869565\n",
      "Most Informative Features\n",
      "                     fda = True                1 : 0      =     13.3 : 1.0\n",
      "                     few = True                1 : 0      =     13.3 : 1.0\n",
      "                      5g = True                1 : 0      =     10.3 : 1.0\n",
      "               emergency = True                1 : 0      =      9.7 : 1.0\n",
      "                   treat = True                1 : 0      =      9.3 : 1.0\n",
      "      hydroxychloroquine = True                1 : 0      =      9.1 : 1.0\n",
      "                    home = True                0 : 1      =      8.4 : 1.0\n",
      "                   right = True                1 : 0      =      8.0 : 1.0\n",
      "                   child = True                1 : 0      =      7.4 : 1.0\n",
      "                   trump = True                1 : 0      =      7.0 : 1.0\n",
      "                    safe = True                0 : 1      =      6.6 : 1.0\n",
      "                 malaria = True                1 : 0      =      5.4 : 1.0\n",
      "               president = True                1 : 0      =      5.4 : 1.0\n",
      "                    drug = True                1 : 0      =      5.2 : 1.0\n",
      "                anything = True                1 : 0      =      4.4 : 1.0\n",
      "                    hoax = True                1 : 0      =      4.4 : 1.0\n",
      "                  report = True                1 : 0      =      4.4 : 1.0\n",
      "                  second = True                1 : 0      =      4.4 : 1.0\n",
      "                     yes = True                1 : 0      =      4.4 : 1.0\n",
      "                   india = True                0 : 1      =      4.3 : 1.0\n",
      "                      up = True                1 : 0      =      4.0 : 1.0\n",
      "                distance = True                0 : 1      =      3.8 : 1.0\n",
      "                actually = True                1 : 0      =      3.8 : 1.0\n",
      "                medicine = True                1 : 0      =      3.8 : 1.0\n",
      "                    stay = True                0 : 1      =      3.6 : 1.0\n",
      "                   agree = True                1 : 0      =      3.4 : 1.0\n",
      "                     bad = True                1 : 0      =      3.4 : 1.0\n",
      "                    best = True                1 : 0      =      3.4 : 1.0\n",
      "                   cause = True                1 : 0      =      3.4 : 1.0\n",
      "                    cell = True                1 : 0      =      3.4 : 1.0\n",
      "None\n",
      "3-gram size of total data  450\n",
      "3-gram size of training data  360\n",
      "3-gram size of test data  90\n",
      "3-gram accuracy for NB: 0.6\n",
      "True info Precision: 0.5930232558139535\n",
      "Misinfo Precision: 0.75\n",
      "True info Recall: 0.9807692307692307\n",
      "Misinfo Recall: 0.07894736842105263\n",
      "True info F-measure: 0.7391304347826086\n",
      "Misinfo F-measure: 0.14285714285714285\n",
      "3-gram accuracy for DT: 0.5888888888888889\n",
      "True info Precision: 0.5842696629213483\n",
      "Misinfo Precision: 1.0\n",
      "True info Recall: 1.0\n",
      "Misinfo Recall: 0.02631578947368421\n",
      "True info F-measure: 0.7375886524822695\n",
      "Misinfo F-measure: 0.05128205128205128\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.450\n",
      "             2          -0.40858        0.997\n",
      "             3          -0.29216        1.000\n",
      "             4          -0.22786        1.000\n",
      "             5          -0.18697        1.000\n",
      "             6          -0.15864        1.000\n",
      "             7          -0.13784        1.000\n",
      "             8          -0.12191        1.000\n",
      "             9          -0.10931        1.000\n",
      "            10          -0.09910        1.000\n",
      "            11          -0.09066        1.000\n",
      "            12          -0.08355        1.000\n",
      "            13          -0.07750        1.000\n",
      "            14          -0.07227        1.000\n",
      "            15          -0.06771        1.000\n",
      "            16          -0.06370        1.000\n",
      "            17          -0.06014        1.000\n",
      "            18          -0.05697        1.000\n",
      "            19          -0.05412        1.000\n",
      "            20          -0.05155        1.000\n",
      "            21          -0.04921        1.000\n",
      "            22          -0.04708        1.000\n",
      "            23          -0.04513        1.000\n",
      "            24          -0.04333        1.000\n",
      "            25          -0.04168        1.000\n",
      "            26          -0.04015        1.000\n",
      "            27          -0.03873        1.000\n",
      "            28          -0.03741        1.000\n",
      "            29          -0.03618        1.000\n",
      "            30          -0.03502        1.000\n",
      "            31          -0.03394        1.000\n",
      "            32          -0.03293        1.000\n",
      "            33          -0.03198        1.000\n",
      "            34          -0.03108        1.000\n",
      "            35          -0.03023        1.000\n",
      "            36          -0.02942        1.000\n",
      "            37          -0.02866        1.000\n",
      "            38          -0.02794        1.000\n",
      "            39          -0.02726        1.000\n",
      "            40          -0.02660        1.000\n",
      "            41          -0.02598        1.000\n",
      "            42          -0.02539        1.000\n",
      "            43          -0.02483        1.000\n",
      "            44          -0.02428        1.000\n",
      "            45          -0.02377        1.000\n",
      "            46          -0.02327        1.000\n",
      "            47          -0.02280        1.000\n",
      "            48          -0.02234        1.000\n",
      "            49          -0.02191        1.000\n",
      "            50          -0.02149        1.000\n",
      "            51          -0.02108        1.000\n",
      "            52          -0.02069        1.000\n",
      "            53          -0.02032        1.000\n",
      "            54          -0.01996        1.000\n",
      "            55          -0.01961        1.000\n",
      "            56          -0.01928        1.000\n",
      "            57          -0.01895        1.000\n",
      "            58          -0.01864        1.000\n",
      "            59          -0.01834        1.000\n",
      "            60          -0.01804        1.000\n",
      "            61          -0.01776        1.000\n",
      "            62          -0.01748        1.000\n",
      "            63          -0.01722        1.000\n",
      "            64          -0.01696        1.000\n",
      "            65          -0.01671        1.000\n",
      "            66          -0.01647        1.000\n",
      "            67          -0.01623        1.000\n",
      "            68          -0.01600        1.000\n",
      "            69          -0.01578        1.000\n",
      "            70          -0.01556        1.000\n",
      "            71          -0.01535        1.000\n",
      "            72          -0.01515        1.000\n",
      "            73          -0.01495        1.000\n",
      "            74          -0.01475        1.000\n",
      "            75          -0.01456        1.000\n",
      "            76          -0.01438        1.000\n",
      "            77          -0.01420        1.000\n",
      "            78          -0.01402        1.000\n",
      "            79          -0.01385        1.000\n",
      "            80          -0.01369        1.000\n",
      "            81          -0.01352        1.000\n",
      "            82          -0.01337        1.000\n",
      "            83          -0.01321        1.000\n",
      "            84          -0.01306        1.000\n",
      "            85          -0.01291        1.000\n",
      "            86          -0.01277        1.000\n",
      "            87          -0.01263        1.000\n",
      "            88          -0.01249        1.000\n",
      "            89          -0.01235        1.000\n",
      "            90          -0.01222        1.000\n",
      "            91          -0.01209        1.000\n",
      "            92          -0.01196        1.000\n",
      "            93          -0.01184        1.000\n",
      "            94          -0.01172        1.000\n",
      "            95          -0.01160        1.000\n",
      "            96          -0.01148        1.000\n",
      "            97          -0.01137        1.000\n",
      "            98          -0.01126        1.000\n",
      "            99          -0.01115        1.000\n",
      "         Final          -0.01104        1.000\n",
      "3-gram accuracy for MXNet: 0.5333333333333333\n",
      "True info Precision: 0.9166666666666666\n",
      "Misinfo Precision: 0.47435897435897434\n",
      "True info Recall: 0.21153846153846154\n",
      "Misinfo Recall: 0.9736842105263158\n",
      "True info F-measure: 0.34375\n",
      "Misinfo F-measure: 0.6379310344827587\n",
      "3-gram accuracy for RF: 0.5888888888888889\n",
      "True info Precision: 0.5842696629213483\n",
      "Misinfo Precision: 1.0\n",
      "True info Recall: 1.0\n",
      "Misinfo Recall: 0.02631578947368421\n",
      "True info F-measure: 0.7375886524822695\n",
      "Misinfo F-measure: 0.05128205128205128\n",
      "3-gram accuracy for SVM: 0.6\n",
      "True info Precision: 0.5909090909090909\n",
      "Misinfo Precision: 1.0\n",
      "True info Recall: 1.0\n",
      "Misinfo Recall: 0.05263157894736842\n",
      "True info F-measure: 0.7428571428571428\n",
      "Misinfo F-measure: 0.1\n",
      "Most Informative Features\n",
      "                     fda = True                1 : 0      =     13.3 : 1.0\n",
      "                     few = True                1 : 0      =     13.3 : 1.0\n",
      "                      5g = True                1 : 0      =     10.3 : 1.0\n",
      "               emergency = True                1 : 0      =      9.7 : 1.0\n",
      "                   treat = True                1 : 0      =      9.3 : 1.0\n",
      "      hydroxychloroquine = True                1 : 0      =      9.1 : 1.0\n",
      "                    home = True                0 : 1      =      8.4 : 1.0\n",
      "                   right = True                1 : 0      =      8.0 : 1.0\n",
      "                   child = True                1 : 0      =      7.4 : 1.0\n",
      "                   trump = True                1 : 0      =      7.0 : 1.0\n",
      "                    safe = True                0 : 1      =      6.6 : 1.0\n",
      "                 malaria = True                1 : 0      =      5.4 : 1.0\n",
      "               president = True                1 : 0      =      5.4 : 1.0\n",
      "                    drug = True                1 : 0      =      5.2 : 1.0\n",
      "                anything = True                1 : 0      =      4.4 : 1.0\n",
      "                    hoax = True                1 : 0      =      4.4 : 1.0\n",
      "                  report = True                1 : 0      =      4.4 : 1.0\n",
      "                  second = True                1 : 0      =      4.4 : 1.0\n",
      "                     yes = True                1 : 0      =      4.4 : 1.0\n",
      "                   india = True                0 : 1      =      4.3 : 1.0\n",
      "                      up = True                1 : 0      =      4.0 : 1.0\n",
      "                distance = True                0 : 1      =      3.8 : 1.0\n",
      "                actually = True                1 : 0      =      3.8 : 1.0\n",
      "                medicine = True                1 : 0      =      3.8 : 1.0\n",
      "                    stay = True                0 : 1      =      3.6 : 1.0\n",
      "                   agree = True                1 : 0      =      3.4 : 1.0\n",
      "                     bad = True                1 : 0      =      3.4 : 1.0\n",
      "                    best = True                1 : 0      =      3.4 : 1.0\n",
      "                   cause = True                1 : 0      =      3.4 : 1.0\n",
      "                    cell = True                1 : 0      =      3.4 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for n in [1,2,3]:\n",
    "    trueinfo_data = []\n",
    "    for i in range(len(true_tweet_dataset)):\n",
    "        words = true_tweet_dataset[i][0]\n",
    "        trueinfo_data.append((create_ngram_features(words, n), \"0\"))    \n",
    "\n",
    "    misinfo_data = []\n",
    "    for i in range(len(misinfo_tweet_dataset)):\n",
    "        words = misinfo_tweet_dataset[i][0]\n",
    "        misinfo_data.append((create_ngram_features(words, n), \"1\")) \n",
    "    trueinfo_data = random.sample(trueinfo_data, 250)\n",
    "    misinfo_data = random.sample(misinfo_data, 200)\n",
    "    Merge_data = trueinfo_data + misinfo_data\n",
    "    print(str(n)+'-gram size of total data ',len(Merge_data))\n",
    "    random.shuffle(Merge_data)\n",
    "    \n",
    "    train_set = Merge_data[:360]\n",
    "    test_set =  Merge_data[360:]\n",
    "    print(str(n)+'-gram size of training data ',len(train_set))\n",
    "    print(str(n)+'-gram size of test data ', len(test_set))\n",
    "    classifier_nb = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    accuracy_nb = nltk.classify.util.accuracy(classifier_nb, test_set)\n",
    "    print(str(n)+'-gram accuracy for NB:', accuracy_nb)\n",
    "    refsets_ngram = collections.defaultdict(set)\n",
    "    testsets_ngram = collections.defaultdict(set)\n",
    "\n",
    "    for i, (feats, label) in enumerate(test_set):\n",
    "        refsets_ngram[label].add(i)\n",
    "        observed = classifier_nb.classify(feats)\n",
    "        testsets_ngram[observed].add(i)\n",
    "    print( 'True info Precision:', precision(refsets_ngram['0'], testsets_ngram['0']))\n",
    "    print( 'Misinfo Precision:', precision(refsets_ngram['1'], testsets_ngram['1']))\n",
    "    print( 'True info Recall:', recall(refsets_ngram['0'], testsets_ngram['0']))\n",
    "    print( 'Misinfo Recall:', recall(refsets_ngram['1'], testsets_ngram['1']) )\n",
    "    print( 'True info F-measure:', f_measure(refsets_ngram['0'], testsets_ngram['0']))\n",
    "    print( 'Misinfo F-measure:', f_measure(refsets_ngram['1'], testsets_ngram['1']) )\n",
    "    \n",
    "    #decision tree\n",
    "    classifier_dt = DecisionTreeClassifier.train(train_set)\n",
    "\n",
    "    accuracy_dt = nltk.classify.util.accuracy(classifier_dt, test_set)\n",
    "    print(str(n)+'-gram accuracy for DT:', accuracy_dt)\n",
    "    \n",
    "    refsets_ngram_dt = collections.defaultdict(set)\n",
    "    testsets_ngram_dt = collections.defaultdict(set)\n",
    "    \n",
    "    for i, (feats, label) in enumerate(test_set):\n",
    "        refsets_ngram_dt[label].add(i)\n",
    "        observed = classifier_dt.classify(feats)\n",
    "        testsets_ngram_dt[observed].add(i)\n",
    "    \n",
    "    print( 'True info Precision:', precision(refsets_ngram_dt['0'], testsets_ngram_dt['0']))\n",
    "    print( 'Misinfo Precision:', precision(refsets_ngram_dt['1'], testsets_ngram_dt['1']))\n",
    "    print( 'True info Recall:', recall(refsets_ngram_dt['0'], testsets_ngram_dt['0']))\n",
    "    print( 'Misinfo Recall:', recall(refsets_ngram_dt['1'], testsets_ngram_dt['1']) )\n",
    "    print( 'True info F-measure:', f_measure(refsets_ngram_dt['0'], testsets_ngram_dt['0']))\n",
    "    print( 'Misinfo F-measure:', f_measure(refsets_ngram_dt['1'], testsets_ngram_dt['1']) )\n",
    "    \n",
    "    #MXnet\n",
    "    classifier_mxn = MaxentClassifier.train(train_set)\n",
    "\n",
    "    accuracy_mxn = nltk.classify.util.accuracy(classifier_mxn, test_set)\n",
    "    print(str(n)+'-gram accuracy for MXNet:', accuracy_mxn)\n",
    "    \n",
    "    refsets_ngram_mxn = collections.defaultdict(set)\n",
    "    testsets_ngram_mxn = collections.defaultdict(set)\n",
    "    \n",
    "    for i, (feats, label) in enumerate(test_set):\n",
    "        refsets_ngram_mxn[label].add(i)\n",
    "        observed = classifier_mxn.classify(feats)\n",
    "        testsets_ngram_mxn[observed].add(i)\n",
    "    \n",
    "    print( 'True info Precision:', precision(refsets_ngram_mxn['0'], testsets_ngram_mxn['0']))\n",
    "    print( 'Misinfo Precision:', precision(refsets_ngram_mxn['1'], testsets_ngram_mxn['1']))\n",
    "    print( 'True info Recall:', recall(refsets_ngram_mxn['0'], testsets_ngram_mxn['0']))\n",
    "    print( 'Misinfo Recall:', recall(refsets_ngram_mxn['1'], testsets_ngram_mxn['1']) )\n",
    "    print( 'True info F-measure:', f_measure(refsets_ngram_mxn['0'], testsets_ngram_mxn['0']))\n",
    "    print( 'Misinfo F-measure:', f_measure(refsets_ngram_mxn['1'], testsets_ngram_mxn['1']) )\n",
    "    \n",
    "    #RandomForest\n",
    "    classifier_rf = SklearnClassifier(RandomForestClassifier()).train(train_set)\n",
    "\n",
    "    accuracy_rf = nltk.classify.util.accuracy(classifier_rf, test_set)\n",
    "    print(str(n)+'-gram accuracy for RF:', accuracy_rf)\n",
    "    \n",
    "    refsets_ngram_rf = collections.defaultdict(set)\n",
    "    testsets_ngram_rf = collections.defaultdict(set)\n",
    "    \n",
    "    for i, (feats, label) in enumerate(test_set):\n",
    "        refsets_ngram_rf[label].add(i)\n",
    "        observed = classifier_rf.classify(feats)\n",
    "        testsets_ngram_rf[observed].add(i)\n",
    "    \n",
    "    print( 'True info Precision:', precision(refsets_ngram_rf['0'], testsets_ngram_rf['0']))\n",
    "    print( 'Misinfo Precision:', precision(refsets_ngram_rf['1'], testsets_ngram_rf['1']))\n",
    "    print( 'True info Recall:', recall(refsets_ngram_rf['0'], testsets_ngram_rf['0']))\n",
    "    print( 'Misinfo Recall:', recall(refsets_ngram_rf['1'], testsets_ngram_rf['1']) )\n",
    "    print( 'True info F-measure:', f_measure(refsets_ngram_rf['0'], testsets_ngram_rf['0']))\n",
    "    print( 'Misinfo F-measure:', f_measure(refsets_ngram_rf['1'], testsets_ngram_rf['1']) )\n",
    "    \n",
    "    #SVM\n",
    "    classifier_svm = SklearnClassifier(SVC()).train(train_set)\n",
    "\n",
    "    accuracy_svm = nltk.classify.util.accuracy(classifier_svm, test_set)\n",
    "    print(str(n)+'-gram accuracy for SVM:', accuracy_svm)\n",
    "    \n",
    "    refsets_ngram_svm = collections.defaultdict(set)\n",
    "    testsets_ngram_svm = collections.defaultdict(set)\n",
    "    \n",
    "    for i, (feats, label) in enumerate(test_set):\n",
    "        refsets_ngram_svm[label].add(i)\n",
    "        observed = classifier_svm.classify(feats)\n",
    "        testsets_ngram_svm[observed].add(i)\n",
    "    \n",
    "    print( 'True info Precision:', precision(refsets_ngram_svm['0'], testsets_ngram_svm['0']))\n",
    "    print( 'Misinfo Precision:', precision(refsets_ngram_svm['1'], testsets_ngram_svm['1']))\n",
    "    print( 'True info Recall:', recall(refsets_ngram_svm['0'], testsets_ngram_svm['0']))\n",
    "    print( 'Misinfo Recall:', recall(refsets_ngram_svm['1'], testsets_ngram_svm['1']) )\n",
    "    print( 'True info F-measure:', f_measure(refsets_ngram_svm['0'], testsets_ngram_svm['0']))\n",
    "    print( 'Misinfo F-measure:', f_measure(refsets_ngram_svm['1'], testsets_ngram_svm['1']) )\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(classifier.show_most_informative_features(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refsets_ngram = collections.defaultdict(set)\n",
    "testsets_ngram = collections.defaultdict(set)\n",
    "\n",
    "for i, (feats, label) in enumerate(test_data):\n",
    "    refsets_ngram[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets_ngram[observed].add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dnspython==2.1.0rc1\n",
      "  Downloading dnspython-2.1.0rc1-py3-none-any.whl (241 kB)\n",
      "Installing collected packages: dnspython\n",
      "Successfully installed dnspython-2.1.0rc1\n"
     ]
    }
   ],
   "source": [
    "!pip install dnspython==2.1.0rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment with CNN/RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp38-cp38-win_amd64.whl (24.2 MB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\mprit\\anaconda3\\lib\\site-packages (from gensim) (1.5.0)\n",
      "Collecting Cython==0.29.14\n",
      "  Downloading Cython-0.29.14-cp38-cp38-win_amd64.whl (1.7 MB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\mprit\\anaconda3\\lib\\site-packages (from gensim) (1.18.5)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-4.0.1.tar.gz (117 kB)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\mprit\\anaconda3\\lib\\site-packages (from gensim) (1.15.0)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-4.0.1-py3-none-any.whl size=108253 sha256=1484d0cf939865f9950a6c6333dab434b4fa086b66fe26a675a5c20a857921b1\n",
      "  Stored in directory: c:\\users\\mprit\\appdata\\local\\pip\\cache\\wheels\\8c\\f9\\f4\\4ddd9ddee3488f48be20e9bf3108961f03ae23da29b7ed26d1\n",
      "Successfully built smart-open\n",
      "Installing collected packages: Cython, smart-open, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.21\n",
      "    Uninstalling Cython-0.29.21:\n",
      "      Successfully uninstalled Cython-0.29.21\n",
      "Successfully installed Cython-0.29.14 gensim-3.8.3 smart-open-4.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\mprit\\anaconda3\\lib\\site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\mprit\\anaconda3\\lib\\site-packages (from keras) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\mprit\\anaconda3\\lib\\site-packages (from keras) (1.18.5)\n",
      "Requirement already satisfied: h5py in c:\\users\\mprit\\anaconda3\\lib\\site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\mprit\\anaconda3\\lib\\site-packages (from h5py->keras) (1.15.0)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.3.1-cp38-cp38-win_amd64.whl (342.5 MB)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\mprit\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.34.0-cp38-cp38-win_amd64.whl (2.9 MB)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in c:\\users\\mprit\\anaconda3\\lib\\site-packages (from tensorflow) (1.18.5)\n",
      "Collecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorboard<3,>=2.3.0\n",
      "  Downloading tensorboard-2.4.0-py3-none-any.whl (10.6 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.14.0-py2.py3-none-any.whl (173 kB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\mprit\\anaconda3\\lib\\site-packages (from tensorflow) (0.34.2)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in c:\\users\\mprit\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\mprit\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\mprit\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\mprit\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (49.2.0.post20200714)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.3-py3-none-any.whl (96 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.23.0-py2.py3-none-any.whl (114 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\mprit\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\mprit\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\mprit\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\mprit\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mprit\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4835 sha256=6b72d8c341530c69b528976246d4c202b5c2add89a3e08f951436c9da473f83a\n",
      "  Stored in directory: c:\\users\\mprit\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built termcolor\n",
      "Installing collected packages: google-pasta, keras-preprocessing, grpcio, astunparse, absl-py, protobuf, markdown, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard-plugin-wit, tensorboard, opt-einsum, termcolor, gast, tensorflow-estimator, tensorflow\n",
      "Successfully installed absl-py-0.11.0 astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 google-auth-1.23.0 google-auth-oauthlib-0.4.2 google-pasta-0.2.0 grpcio-1.34.0 keras-preprocessing-1.1.2 markdown-3.3.3 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.14.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.6 tensorboard-2.4.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.1 tensorflow-estimator-2.3.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
